From fe762161105a003b0379f1be4776378986a3d1fc Mon Sep 17 00:00:00 2001
From: cnauroth <cnauroth@apache.org>
Date: Fri, 20 Feb 2015 12:38:34 -0800
Subject: [PATCH 0837/1023] HDFS-7773. Additional metrics in HDFS to be
 accessed via jmx. Contributed by Anu Engineer.

(cherry picked from commit e09ba94f28286be1597bdec27e1679bd812c3819)

Change-Id: Ibd2e12a233cb27c4beeff9ff3c9fa75c2703b454
---
 .../hadoop-common/src/site/apt/Metrics.apt.vm      |   13 ++++++
 .../hadoop/hdfs/server/datanode/BlockReceiver.java |    1 +
 .../hadoop/hdfs/server/datanode/DataXceiver.java   |   26 +++++++----
 .../server/datanode/metrics/DataNodeMetrics.java   |   36 ++++++++++++++--
 .../server/namenode/metrics/NameNodeMetrics.java   |   24 +++++++++++
 .../hdfs/server/datanode/TestDataNodeMetrics.java  |   45 ++++++++++++++++++++
 .../namenode/metrics/TestNameNodeMetrics.java      |   20 +++++++++
 7 files changed, 152 insertions(+), 13 deletions(-)

diff --git a/hadoop-common-project/hadoop-common/src/site/apt/Metrics.apt.vm b/hadoop-common-project/hadoop-common/src/site/apt/Metrics.apt.vm
index 14cc712..8e81b50 100644
--- a/hadoop-common-project/hadoop-common/src/site/apt/Metrics.apt.vm
+++ b/hadoop-common-project/hadoop-common/src/site/apt/Metrics.apt.vm
@@ -336,6 +336,8 @@ dfs context
 *-------------------------------------+--------------------------------------+
 |<<<PutImageAvgTime>>> | Average fsimage upload time in milliseconds
 *-------------------------------------+--------------------------------------+
+|<<<TotalFileOps>>> | Total number of file operations performed
+*-------------------------------------+--------------------------------------+
 
 * FSNamesystem
 
@@ -604,6 +606,17 @@ dfs context
 |<<<SendDataPacketTransferNanosAvgTime>>> | Average transfer time of sending
                                           | packets in nanoseconds
 *-------------------------------------+--------------------------------------+
+|<<<TotalWriteTime>>> | Total number of milliseconds spent on write
+                      | operation
+*-------------------------------------+--------------------------------------+
+|<<<TotalReadTime>>> | Total number of milliseconds spent on read
+                     | operation
+*-------------------------------------+--------------------------------------+
+|<<<RemoteBytesRead>>> | Number of bytes read by remote clients
+*-------------------------------------+--------------------------------------+
+|<<<RemoteBytesWritten>>> | Number of bytes written by remote clients
+*-------------------------------------+--------------------------------------+
+
 
 ugi context
 
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java
index 1f69397..6ac8a78 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java
@@ -652,6 +652,7 @@ private int receivePacket() throws IOException {
           replicaInfo.setLastChecksumAndDataLen(offsetInBlock, lastCrc);
 
           datanode.metrics.incrBytesWritten(len);
+          datanode.metrics.incrTotalWriteTime(duration);
 
           manageWriterOsCache(offsetInBlock);
         }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java
index 782667f..d2f9b4f 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java
@@ -88,6 +88,7 @@
 
 import com.google.common.base.Preconditions;
 import com.google.protobuf.ByteString;
+import org.apache.hadoop.util.Time;
 
 
 /**
@@ -504,7 +505,7 @@ public void readBlock(final ExtendedBlock block,
       final boolean sendChecksum,
       final CachingStrategy cachingStrategy) throws IOException {
     previousOpClientName = clientName;
-
+    long read = 0;
     OutputStream baseStream = getOutputStream();
     DataOutputStream out = new DataOutputStream(new BufferedOutputStream(
         baseStream, HdfsConstants.SMALL_BUFFER_SIZE));
@@ -539,8 +540,9 @@ public void readBlock(final ExtendedBlock block,
       // send op status
       writeSuccessWithChecksumInfo(blockSender, new DataOutputStream(getOutputStream()));
 
-      long read = blockSender.sendBlock(out, baseStream, null); // send data
-
+      long beginRead = Time.monotonicNow();
+      read = blockSender.sendBlock(out, baseStream, null); // send data
+      long duration = Time.monotonicNow() - beginRead;
       if (blockSender.didSendEntireByteRange()) {
         // If we sent the entire range, then we should expect the client
         // to respond with a Status enum.
@@ -563,6 +565,7 @@ public void readBlock(final ExtendedBlock block,
       }
       datanode.metrics.incrBytesRead((int) read);
       datanode.metrics.incrBlocksRead();
+      datanode.metrics.incrTotalReadTime(duration);
     } catch ( SocketException ignored ) {
       if (LOG.isTraceEnabled()) {
         LOG.trace(dnR + ":Ignoring exception while serving " + block + " to " +
@@ -587,7 +590,7 @@ public void readBlock(final ExtendedBlock block,
 
     //update metrics
     datanode.metrics.addReadBlockOp(elapsed());
-    datanode.metrics.incrReadsFromClient(peer.isLocal());
+    datanode.metrics.incrReadsFromClient(peer.isLocal(), read);
   }
 
   @Override
@@ -612,7 +615,7 @@ public void writeBlock(final ExtendedBlock block,
     final boolean isClient = !isDatanode;
     final boolean isTransfer = stage == BlockConstructionStage.TRANSFER_RBW
         || stage == BlockConstructionStage.TRANSFER_FINALIZED;
-
+    long size = 0;
     // check single target for transfer-RBW/Finalized 
     if (isTransfer && targets.length > 0) {
       throw new IOException(stage + " does not support multiple targets "
@@ -809,7 +812,9 @@ public void writeBlock(final ExtendedBlock block,
             + localAddress + " of size " + block.getNumBytes());
       }
 
-      
+      if(isClient) {
+        size = block.getNumBytes();
+      }
     } catch (IOException ioe) {
       LOG.info("opWriteBlock " + block + " received exception " + ioe);
       incrDatanodeNetworkErrors();
@@ -826,7 +831,7 @@ public void writeBlock(final ExtendedBlock block,
 
     //update metrics
     datanode.metrics.addWriteBlockOp(elapsed());
-    datanode.metrics.incrWritesFromClient(peer.isLocal());
+    datanode.metrics.incrWritesFromClient(peer.isLocal(), size);
   }
 
   @Override
@@ -999,12 +1004,15 @@ public void copyBlock(final ExtendedBlock block,
 
       // send status first
       writeSuccessWithChecksumInfo(blockSender, reply);
+
+      long beginRead = Time.monotonicNow();
       // send block content to the target
-      long read = blockSender.sendBlock(reply, baseStream, 
+      long read = blockSender.sendBlock(reply, baseStream,
                                         dataXceiverServer.balanceThrottler);
-
+      long duration = Time.monotonicNow() - beginRead;
       datanode.metrics.incrBytesRead((int) read);
       datanode.metrics.incrBlocksRead();
+      datanode.metrics.incrTotalReadTime(duration);
       
       LOG.info("Copied " + block + " to " + peer.getRemoteAddressString());
     } catch (IOException ioe) {
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeMetrics.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeMetrics.java
index ed001c8..2e8eb22 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeMetrics.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeMetrics.java
@@ -50,7 +50,11 @@
 public class DataNodeMetrics {
 
   @Metric MutableCounterLong bytesWritten;
+  @Metric("Milliseconds spent writing")
+  MutableCounterLong totalWriteTime;
   @Metric MutableCounterLong bytesRead;
+  @Metric("Milliseconds spent reading")
+  MutableCounterLong totalReadTime;
   @Metric MutableCounterLong blocksWritten;
   @Metric MutableCounterLong blocksRead;
   @Metric MutableCounterLong blocksReplicated;
@@ -64,6 +68,10 @@
   @Metric MutableCounterLong writesFromLocalClient;
   @Metric MutableCounterLong writesFromRemoteClient;
   @Metric MutableCounterLong blocksGetLocalPathInfo;
+  @Metric("Bytes read by remote client")
+  MutableCounterLong remoteBytesRead;
+  @Metric("Bytes written by remote client")
+  MutableCounterLong remoteBytesWritten;
 
   // RamDisk metrics on read/write
   @Metric MutableCounterLong ramDiskBlocksWrite;
@@ -267,6 +275,15 @@ public void incrFsyncCount() {
     fsyncCount.incr();
   }
 
+  public void incrTotalWriteTime(long timeTaken) {
+    totalWriteTime.incr(timeTaken);
+  }
+
+  public void incrTotalReadTime(long timeTaken) {
+    totalReadTime.incr(timeTaken);
+  }
+
+
   public void addPacketAckRoundTripTimeNanos(long latencyNanos) {
     packetAckRoundTripTimeNanos.add(latencyNanos);
     for (MutableQuantiles q : packetAckRoundTripTimeNanosQuantiles) {
@@ -292,12 +309,23 @@ public void shutdown() {
     DefaultMetricsSystem.shutdown();
   }
 
-  public void incrWritesFromClient(boolean local) {
-    (local ? writesFromLocalClient : writesFromRemoteClient).incr();
+  public void incrWritesFromClient(boolean local, long size) {
+    if(local) {
+      writesFromLocalClient.incr();
+    } else {
+      writesFromRemoteClient.incr();
+      remoteBytesWritten.incr(size);
+    }
   }
 
-  public void incrReadsFromClient(boolean local) {
-    (local ? readsFromLocalClient : readsFromRemoteClient).incr();
+  public void incrReadsFromClient(boolean local, long size) {
+
+    if (local) {
+      readsFromLocalClient.incr();
+    } else {
+      readsFromRemoteClient.incr();
+      remoteBytesRead.incr(size);
+    }
   }
   
   public void incrVolumeFailures() {
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeMetrics.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeMetrics.java
index 42942dc..2d49414 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeMetrics.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeMetrics.java
@@ -76,6 +76,30 @@
   @Metric("Number of blockReports from individual storages")
   MutableCounterLong storageBlockReportOps;
 
+  @Metric("Number of file system operations")
+  public long totalFileOps(){
+    return
+      getBlockLocations.value() +
+      createFileOps.value() +
+      filesAppended.value() +
+      addBlockOps.value() +
+      getAdditionalDatanodeOps.value() +
+      filesRenamed.value() +
+      deleteFileOps.value() +
+      getListingOps.value() +
+      fileInfoOps.value() +
+      getLinkTargetOps.value() +
+      createSnapshotOps.value() +
+      deleteSnapshotOps.value() +
+      allowSnapshotOps.value() +
+      disallowSnapshotOps.value() +
+      renameSnapshotOps.value() +
+      listSnapshottableDirOps.value() +
+      createSymlinkOps.value() +
+      snapshotDiffReportOps.value();
+  }
+
+
   @Metric("Journal transactions") MutableRate transactions;
   @Metric("Journal syncs") MutableRate syncs;
   final MutableQuantiles[] syncsQuantiles;
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeMetrics.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeMetrics.java
index 95d90cb..5d27fe6 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeMetrics.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeMetrics.java
@@ -47,6 +47,7 @@
 import org.apache.hadoop.hdfs.protocol.DatanodeInfo;
 import org.apache.hadoop.io.IOUtils;
 import org.apache.hadoop.metrics2.MetricsRecordBuilder;
+import org.apache.hadoop.util.Time;
 import org.junit.Test;
 import org.mockito.Mockito;
 
@@ -248,4 +249,48 @@ public void testTimeoutMetric() throws Exception {
       DataNodeFaultInjector.instance = new DataNodeFaultInjector();
     }
   }
+
+  /**
+   * This function ensures that writing causes TotalWritetime to increment
+   * and reading causes totalReadTime to move.
+   * @throws Exception
+   */
+  @Test
+  public void testDataNodeTimeSpend() throws Exception {
+    Configuration conf = new HdfsConfiguration();
+    SimulatedFSDataset.setFactory(conf);
+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).build();
+    try {
+      FileSystem fs = cluster.getFileSystem();
+      List<DataNode> datanodes = cluster.getDataNodes();
+      assertEquals(datanodes.size(), 1);
+      DataNode datanode = datanodes.get(0);
+      MetricsRecordBuilder rb = getMetrics(datanode.getMetrics().name());
+      final long LONG_FILE_LEN = 1024 * 1024 * 10;
+
+      long startWriteValue = getLongCounter("TotalWriteTime", rb);
+      long startReadValue = getLongCounter("TotalReadTime", rb);
+
+      for (int x =0; x < 50; x++) {
+        DFSTestUtil.createFile(fs, new Path("/time.txt."+ x),
+                LONG_FILE_LEN, (short) 1, Time.monotonicNow());
+      }
+
+      for (int x =0; x < 50; x++) {
+        String s = DFSTestUtil.readFile(fs, new Path("/time.txt." + x));
+      }
+
+      MetricsRecordBuilder rbNew = getMetrics(datanode.getMetrics().name());
+      long endWriteValue = getLongCounter("TotalWriteTime", rbNew);
+      long endReadValue = getLongCounter("TotalReadTime", rbNew);
+
+      assertTrue(endReadValue > startReadValue);
+      assertTrue(endWriteValue > startWriteValue);
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
 }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/metrics/TestNameNodeMetrics.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/metrics/TestNameNodeMetrics.java
index a1b04ac..8abc15e 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/metrics/TestNameNodeMetrics.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/metrics/TestNameNodeMetrics.java
@@ -545,4 +545,24 @@ public void testSyncAndBlockReportMetric() throws Exception {
     assertQuantileGauges("Syncs1s", rb);
     assertQuantileGauges("BlockReport1s", rb);
   }
+
+  /**
+   * Test NN ReadOps Count and WriteOps Count
+   */
+  @Test
+  public void testReadWriteOps() throws Exception {
+    MetricsRecordBuilder rb = getMetrics(NN_METRICS);
+    long startWriteCounter = MetricsAsserts.getLongCounter("TransactionsNumOps",
+        rb);
+    Path file1_Path = new Path(TEST_ROOT_DIR_PATH, "ReadData.dat");
+
+    //Perform create file operation
+    createFile(file1_Path, 1024 * 1024,(short)2);
+
+    // Perform read file operation on earlier created file
+    readFile(fs, file1_Path);
+    MetricsRecordBuilder rbNew = getMetrics(NN_METRICS);
+    assertTrue(MetricsAsserts.getLongCounter("TransactionsNumOps", rbNew) >
+        startWriteCounter);
+  }
 }
-- 
1.7.9.5

